{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import time\n",
    "import sys\n",
    "import csv\n",
    "from operator import itemgetter\n",
    "import time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, BaggingRegressor, GradientBoostingRegressor, AdaBoostRegressor, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_selection import RFE, VarianceThreshold, SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression, mutual_info_classif, chi2\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from sklearn.preprocessing import KBinsDiscretizer, scale\n",
    "\n",
    "#Handle annoying warnings\n",
    "import warnings, sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#\n",
    "# Global parameters\n",
    "#\n",
    "#####################\n",
    "\n",
    "target_idx=0                                        #Index of Target variable\n",
    "cross_val=1                                         #Control Switch for CV\n",
    "norm_target=1                                       #Normalize target switch\n",
    "norm_features=1                                     #Normalize target switch\n",
    "binning=0                                           #Control Switch for Bin Target\n",
    "bin_cnt=2                                           #If bin target, this sets number of classes\n",
    "feat_select=1                                       #Control Switch for Feature Selection\n",
    "fs_type=2                                           #Feature Selection type (1=Stepwise Backwards Removal, 2=Wrapper Select, 3=Univariate Selection)\n",
    "lv_filter=0                                         #Control switch for low variance filter on features\n",
    "feat_start=1                                        #Start column of features\n",
    "k_cnt=5                                             #Number of 'Top k' best ranked features to select, only applies for fs_types 1 and 3\n",
    "\n",
    "#Set global model parameters\n",
    "rand_st=0                                           #Set Random State variable for randomizing splits on runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï»¿avg_rating', 'revenue', 'budget', 'popularity', 'vote_average', 'vote_count', 'avg_rating5', 'avg_rating10', 'avg_rating15', 'avg_rating20', 'Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'IMAX', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western', 'user1', 'user2', 'user3', 'user4', 'user5', 'user6', 'user7', 'user8', 'user9', 'user10']\n",
      "2750 2750\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "#\n",
    "# Load Data\n",
    "#\n",
    "#####################\n",
    "\n",
    "file1= csv.reader(open('C:/Users/arito/Documents/DePaul/DSC540/Final Project/data/model data/movie3_dum_collab.csv'), \n",
    "                  delimiter=',', quotechar='\"')\n",
    "\n",
    "#Read Header Line\n",
    "header=next(file1)            \n",
    "\n",
    "#Read data\n",
    "data=[]\n",
    "target=[]\n",
    "for row in file1:\n",
    "    #Load Target\n",
    "    if row[target_idx]=='':                         #If target is blank, skip row                       \n",
    "        continue\n",
    "    else:\n",
    "        target.append(float(row[target_idx]))       #If pre-binned class, change float to int\n",
    "\n",
    "    #Load row into temp array, cast columns  \n",
    "    temp=[]\n",
    "                 \n",
    "    for j in range(feat_start,len(header)):\n",
    "        if row[j]=='':\n",
    "            temp.append(float())\n",
    "        else:\n",
    "            temp.append(float(row[j]))\n",
    "\n",
    "    #Load temp into Data array\n",
    "    data.append(temp)\n",
    "  \n",
    "#Test Print\n",
    "print(header)\n",
    "print(len(target),len(data))\n",
    "'''for i in range(10):\n",
    "    print(target[i])\n",
    "    print(data[i])'''\n",
    "print('\\n')\n",
    "\n",
    "data_np=np.asarray(data)\n",
    "target_np=np.asarray(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arito\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:180: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--FEATURE SELECTION ON-- \n",
      "\n",
      "Wrapper Select - Random Forest: \n",
      "Selected ['vote_average', 'avg_rating5', 'avg_rating10', 'avg_rating15', 'avg_rating20', 'user1', 'user4', 'user5']\n",
      "Features (total/selected): 38 8\n",
      "\n",
      "\n",
      "--ML Model Output-- \n",
      "\n",
      "Decision Tree RMSE:: 0.63 (+/- 0.09)\n",
      "Decision Tree Expl Var: 0.60 (+/- 0.18)\n",
      "CV Runtime: 0.06988763809204102\n",
      "Random Forest RMSE:: 0.45 (+/- 0.06)\n",
      "Random Forest Expl Var: 0.81 (+/- 0.03)\n",
      "CV Runtime: 1.7562274932861328\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "#\n",
    "# Preprocess data\n",
    "#\n",
    "##########################################\n",
    "\n",
    "\n",
    "if norm_target==1:\n",
    "    #Target normalization for continuous values\n",
    "    target_np=scale(target_np)\n",
    "\n",
    "if norm_features==1:\n",
    "    #Feature normalization for continuous values\n",
    "    data_np=scale(data_np)\n",
    "\n",
    "if binning==1:\n",
    "    #Discretize Target variable with KBinsDiscretizer\n",
    "    enc = KBinsDiscretizer(n_bins=[bin_cnt], encode='ordinal', strategy='quantile')                         #Strategy here is important, quantile creating equal bins, but kmeans prob being more valid \"clusters\"\n",
    "    target_np_bin = enc.fit_transform(target_np.reshape(-1,1))\n",
    "\n",
    "    #Get Bin min/max\n",
    "    temp=[[] for x in range(bin_cnt+1)]\n",
    "    for i in range(len(target_np)):\n",
    "        for j in range(bin_cnt):\n",
    "            if target_np_bin[i]==j:\n",
    "                temp[j].append(target_np[i])\n",
    "\n",
    "    for j in range(bin_cnt):\n",
    "        print('Bin', j, ':', min(temp[j]), max(temp[j]), len(temp[j]))\n",
    "    print('\\n')\n",
    "\n",
    "    #Convert Target array back to correct shape\n",
    "    target_np=np.ravel(target_np_bin)\n",
    "\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#\n",
    "# Feature Selection\n",
    "#\n",
    "##########################################\n",
    "\n",
    "#Low Variance Filter\n",
    "if lv_filter==1:\n",
    "    print('--LOW VARIANCE FILTER ON--', '\\n')\n",
    "    \n",
    "    #LV Threshold\n",
    "    sel = VarianceThreshold(threshold=0.5)                                      #Removes any feature with less than 20% variance\n",
    "    fit_mod=sel.fit(data_np)\n",
    "    fitted=sel.transform(data_np)\n",
    "    sel_idx=fit_mod.get_support()\n",
    "\n",
    "    #Get lists of selected and non-selected features (names and indexes)\n",
    "    temp=[]\n",
    "    temp_idx=[]\n",
    "    temp_del=[]\n",
    "    for i in range(len(data_np[0])):\n",
    "        if sel_idx[i]==1:                                                           #Selected Features get added to temp header\n",
    "            temp.append(header[i+feat_start])\n",
    "            temp_idx.append(i)\n",
    "        else:                                                                       #Indexes of non-selected features get added to delete array\n",
    "            temp_del.append(i)\n",
    "\n",
    "    print('Selected', temp)\n",
    "    print('Features (total, selected):', len(data_np[0]), len(temp))\n",
    "    print('\\n')\n",
    "\n",
    "    #Filter selected columns from original dataset\n",
    "    header = header[0:feat_start]\n",
    "    for field in temp:\n",
    "        header.append(field)\n",
    "    data_np = np.delete(data_np, temp_del, axis=1)                                 #Deletes non-selected features by index\n",
    "\n",
    "\n",
    "#Feature Selection\n",
    "if feat_select==1:\n",
    "    '''Three steps:\n",
    "       1) Run Feature Selection\n",
    "       2) Get lists of selected and non-selected features\n",
    "       3) Filter columns from original dataset\n",
    "       '''\n",
    "    \n",
    "    print('--FEATURE SELECTION ON--', '\\n')\n",
    "    \n",
    "    ##1) Run Feature Selection #######\n",
    "    if fs_type==1:\n",
    "        #Stepwise Recursive Backwards Feature removal\n",
    "        if binning==1:\n",
    "            clf = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=3, criterion='entropy', random_state=None)\n",
    "            sel = RFE(clf, n_features_to_select=k_cnt, step=.1)\n",
    "            print('Stepwise Recursive Backwards - Random Forest: ')\n",
    "        if binning==0:\n",
    "            rgr = RandomForestRegressor(n_estimators=500, max_depth=None, min_samples_split=3, criterion='mse', random_state=None)\n",
    "            sel = RFE(rgr, n_features_to_select=k_cnt, step=.1)\n",
    "            print('Stepwise Recursive Backwards - Random Forest: ')\n",
    "            \n",
    "        fit_mod=sel.fit(data_np, target_np)\n",
    "        print(sel.ranking_)\n",
    "        sel_idx=fit_mod.get_support()      \n",
    "\n",
    "    if fs_type==2:\n",
    "        #Wrapper Select via model\n",
    "        if binning==1:\n",
    "            clf = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=3, criterion='entropy', random_state=None)\n",
    "            sel = SelectFromModel(clf, prefit=False, threshold='mean', max_features=None)                                                           #to select only based on max_features, set to integer value and set threshold=-np.inf\n",
    "            print ('Wrapper Select - Random Forest: ')\n",
    "        if binning==0:\n",
    "            rgr = RandomForestRegressor(n_estimators=500, max_features=.33, max_depth=None, min_samples_split=3, criterion='mse', random_state=None)\n",
    "            sel = SelectFromModel(rgr, prefit=False, threshold='mean', max_features=None)\n",
    "            print ('Wrapper Select - Random Forest: ')\n",
    "            \n",
    "        fit_mod=sel.fit(data_np, target_np)    \n",
    "        sel_idx=fit_mod.get_support()\n",
    "\n",
    "    if fs_type==3:       \n",
    "        if binning==1:                                                              ######Only work if the Target is binned###########\n",
    "            #Univariate Feature Selection - Chi-squared\n",
    "            sel=SelectKBest(chi2, k=k_cnt)\n",
    "            fit_mod=sel.fit(data_np, target_np)                                         #will throw error if any negative values in features, so turn off feature normalization, or switch to mutual_info_classif\n",
    "            print ('Univariate Feature Selection - Chi2: ')\n",
    "            sel_idx=fit_mod.get_support()\n",
    "\n",
    "        if binning==0:                                                              ######Only work if the Target is continuous###########\n",
    "            #Univariate Feature Selection - Mutual Info Regression\n",
    "            sel=SelectKBest(mutual_info_regression, k=k_cnt)\n",
    "            fit_mod=sel.fit(data_np, target_np)\n",
    "            print ('Univariate Feature Selection - Mutual Info: ')\n",
    "            sel_idx=fit_mod.get_support()\n",
    "\n",
    "        #Print ranked variables out sorted\n",
    "        temp=[]\n",
    "        scores=fit_mod.scores_\n",
    "        for i in range(feat_start, len(header)):            \n",
    "            temp.append([header[i], float(scores[i-feat_start])])\n",
    "\n",
    "        print('Ranked Features')\n",
    "        temp_sort=sorted(temp, key=itemgetter(1), reverse=True)                     #Doesn't always sort correctly (e.g. for Chi Sq), so doublecheck output\n",
    "        for i in range(len(temp_sort)):\n",
    "            print(i, temp_sort[i][0], ':', temp_sort[i][1])\n",
    "        print('\\n')\n",
    "\n",
    "    ##2) Get lists of selected and non-selected features (names and indexes) #######\n",
    "    temp=[]\n",
    "    temp_idx=[]\n",
    "    temp_del=[]\n",
    "    for i in range(len(data_np[0])):\n",
    "        if sel_idx[i]==1:                                                           #Selected Features get added to temp header\n",
    "            temp.append(header[i+feat_start])\n",
    "            temp_idx.append(i)\n",
    "        else:                                                                       #Indexes of non-selected features get added to delete array\n",
    "            temp_del.append(i)\n",
    "    print('Selected', temp)\n",
    "    print('Features (total/selected):', len(data_np[0]), len(temp))\n",
    "    print('\\n')\n",
    "            \n",
    "                \n",
    "    ##3) Filter selected columns from original dataset #########\n",
    "    header = header[0:feat_start]\n",
    "    for field in temp:\n",
    "        header.append(field)\n",
    "    data_np = np.delete(data_np, temp_del, axis=1)                                 #Deletes non-selected features by index)\n",
    "\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#\n",
    "# Train SciKit Models\n",
    "#\n",
    "##########################################\n",
    "\n",
    "print('--ML Model Output--', '\\n')\n",
    "\n",
    "#Test/Train split\n",
    "data_train, data_test, target_train, target_test = train_test_split(data_np, target_np, test_size=0.35)\n",
    "\n",
    "####Classifiers####\n",
    "if binning==1 and cross_val==0:    \n",
    "    #SciKit Decision Tree\n",
    "    clf = DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=None, min_samples_split=3, min_samples_leaf=1, max_features=None, random_state=None)\n",
    "    clf.fit(data_train, target_train)\n",
    "    print('Decision Tree Acc:', clf.score(data_test, target_test))\n",
    "    if bin_cnt<=2:                                                                                                  #AUC only works with binary classes, not multiclass\n",
    "        print('Decision Tree AUC:', metrics.roc_auc_score(target_test, clf.predict_proba(data_test)[:,1]))             \n",
    "    #joblib.dump(clf, 'DecTree_DSC540_HW1.pkl')                     #Save and pickle model\n",
    "\n",
    "    #SciKit Random Forest\n",
    "    clf = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=3, criterion='entropy', random_state=None)\n",
    "    clf.fit(data_train, target_train)\n",
    "    print('Random Forest Acc:', clf.score(data_test, target_test))\n",
    "    if bin_cnt<=2:                                                                                                  #AUC only works with binary classes, not multiclass\n",
    "        print('Random Forest AUC:', metrics.roc_auc_score(target_test, clf.predict_proba(data_test)[:,1]))             \n",
    " \n",
    "####Regressors####\n",
    "if binning==0 and cross_val==0:\n",
    "    #SciKit Decision Tree Regressor\n",
    "    rgr = DecisionTreeRegressor(criterion='mse', splitter='best', max_depth=None, min_samples_split=3, min_samples_leaf=1, max_features=None, random_state=None)\n",
    "    rgr.fit(data_train, target_train)\n",
    "    print('Decision Tree RMSE:', math.sqrt(metrics.mean_squared_error(target_test, rgr.predict(data_test))))\n",
    "    print('Decistion Tree Expl Var:', metrics.explained_variance_score(target_test, rgr.predict(data_test)))\n",
    "\n",
    "    #SciKit Random Forest Regressor\n",
    "    rgr = RandomForestRegressor(n_estimators=500, max_features=.33, max_depth=None, min_samples_split=3, criterion='mse', random_state=None)\n",
    "    rgr.fit(data_train, target_train)\n",
    "    print('Random Forest RMSE:', math.sqrt(metrics.mean_squared_error(target_test, rgr.predict(data_test))))\n",
    "    print('Random Forest Expl Var:', metrics.explained_variance_score(target_test, rgr.predict(data_test)))\n",
    "\n",
    "\n",
    "####Cross-Val Classifiers####\n",
    "if binning==1 and cross_val==1:\n",
    "    #Setup Crossval classifier scorers\n",
    "    if bin_cnt<=2:\n",
    "        scorers = {'Accuracy': 'accuracy', 'roc_auc': 'roc_auc'}\n",
    "    else:\n",
    "        scorers = {'Accuracy': 'accuracy'}\n",
    "    \n",
    "    #SciKit Decision Tree - Cross Val\n",
    "    start_ts=time.time()\n",
    "    clf = DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=None, min_samples_split=3, min_samples_leaf=1, max_features=None, random_state=None)\n",
    "    scores = cross_validate(clf, data_np, target_np, scoring=scorers, cv=5)\n",
    "    scores_Acc = scores['test_Accuracy']\n",
    "    print(\"Decision Tree Acc: %0.2f (+/- %0.2f)\" % (scores_Acc.mean(), scores_Acc.std() * 2))        \n",
    "    if bin_cnt<=2:                                                                                                  #Only works with binary classes, not multiclass\n",
    "        scores_AUC= scores['test_roc_auc']\n",
    "        print(\"Decision Tree AUC: %0.2f (+/- %0.2f)\" % (scores_AUC.mean(), scores_AUC.std() * 2))                           \n",
    "    print(\"CV Runtime:\", time.time()-start_ts)\n",
    "\n",
    "    #SciKit Random Forest - Cross Val\n",
    "    start_ts=time.time()\n",
    "    clf = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=3, criterion='entropy', random_state=None)\n",
    "    scores = cross_validate(clf, data_np, target_np, scoring=scorers, cv=5)\n",
    "    scores_Acc = scores['test_Accuracy']\n",
    "    print(\"Random Forest Acc: %0.2f (+/- %0.2f)\" % (scores_Acc.mean(), scores_Acc.std() * 2))        \n",
    "    if bin_cnt<=2:                                                                                                  #Only works with binary classes, not multiclass\n",
    "        scores_AUC= scores['test_roc_auc']\n",
    "        print(\"Random Forest AUC: %0.2f (+/- %0.2f)\" % (scores_AUC.mean(), scores_AUC.std() * 2))                           \n",
    "    print(\"CV Runtime:\", time.time()-start_ts)\n",
    "\n",
    "\n",
    "####Cross-Val Regressors####\n",
    "if binning==0 and cross_val==1:\n",
    "    #Setup Crossval regression scorers\n",
    "    scorers = {'Neg_MSE': 'neg_mean_squared_error', 'expl_var': 'explained_variance'} \n",
    "    \n",
    "    #SciKit Decision Tree Regressor - Cross Val\n",
    "    start_ts=time.time()\n",
    "    rgr = DecisionTreeRegressor(criterion='mse', splitter='best', max_depth=None, min_samples_split=3, min_samples_leaf=1, max_features=None, random_state=None)\n",
    "    scores = cross_validate(rgr, data_np, target_np, scoring=scorers, cv=5)\n",
    "    scores_RMSE = np.asarray([math.sqrt(-x) for x in scores['test_Neg_MSE']])                                       #Turns negative MSE scores into RMSE\n",
    "    scores_Expl_Var = scores['test_expl_var']\n",
    "    print(\"Decision Tree RMSE:: %0.2f (+/- %0.2f)\" % ((scores_RMSE.mean()), (scores_RMSE.std() * 2)))\n",
    "    print(\"Decision Tree Expl Var: %0.2f (+/- %0.2f)\" % ((scores_Expl_Var.mean()), (scores_Expl_Var.std() * 2)))\n",
    "    print(\"CV Runtime:\", time.time()-start_ts)\n",
    "\n",
    "    #SciKit Random Forest Regressor - Cross Val\n",
    "    start_ts=time.time()\n",
    "    rgr = RandomForestRegressor(n_estimators=100, max_features=.33, max_depth=None, min_samples_split=3, criterion='mse', random_state=None)\n",
    "    scores = cross_validate(rgr, data_np, target_np, scoring=scorers, cv=5)\n",
    "    scores_RMSE = np.asarray([math.sqrt(-x) for x in scores['test_Neg_MSE']])                                       #Turns negative MSE scores into RMSE\n",
    "    scores_Expl_Var = scores['test_expl_var']\n",
    "    print(\"Random Forest RMSE:: %0.2f (+/- %0.2f)\" % ((scores_RMSE.mean()), (scores_RMSE.std() * 2)))\n",
    "    print(\"Random Forest Expl Var: %0.2f (+/- %0.2f)\" % ((scores_Expl_Var.mean()), (scores_Expl_Var.std() * 2)))\n",
    "    print(\"CV Runtime:\", time.time()-start_ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--FEATURE SELECTION ON-- \n",
      "\n",
      "Wrapper Select: \n",
      "Selected ['vote_average']\n",
      "Features (total/selected): 8 1\n",
      "\n",
      "\n",
      "--ML Model Output-- \n",
      "\n",
      "Gradient Boosting RMSE:: 0.51 (+/- 0.07)\n",
      "Gradient Boosting Expl Var: 0.74 (+/- 0.04)\n",
      "Gradient Boosting CV Runtime: 0.17353391647338867\n",
      "AdaBoost RMSE:: 0.54 (+/- 0.06)\n",
      "AdaBoost Expl Var: 0.73 (+/- 0.03)\n",
      "AdaBoost CV Runtime: 0.22539901733398438\n",
      "Neural Network RMSE:: 0.49 (+/- 0.08)\n",
      "Neural Network Expl Var: 0.75 (+/- 0.04)\n",
      "Neural Network CV Runtime: 0.5449948310852051\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "#\n",
    "# Preprocess data\n",
    "#\n",
    "##########################################\n",
    "\n",
    "if norm_target==1:\n",
    "    #Target normalization for continuous values\n",
    "    target_np=scale(target_np)\n",
    "\n",
    "if norm_features==1:\n",
    "    #Feature normalization for continuous values\n",
    "    data_np=scale(data_np)\n",
    "\n",
    "if binning==1:\n",
    "    #Discretize Target variable with KBinsDiscretizer\n",
    "    enc = KBinsDiscretizer(n_bins=[bin_cnt], encode='ordinal', strategy='quantile')                         #Strategy here is important, quantile creating equal bins, but kmeans prob being more valid \"clusters\"\n",
    "    target_np_bin = enc.fit_transform(target_np.reshape(-1,1))\n",
    "\n",
    "    #Get Bin min/max\n",
    "    temp=[[] for x in range(bin_cnt+1)]\n",
    "    for i in range(len(target_np)):\n",
    "        for j in range(bin_cnt):\n",
    "            if target_np_bin[i]==j:\n",
    "                temp[j].append(target_np[i])\n",
    "\n",
    "    for j in range(bin_cnt):\n",
    "        print('Bin', j, ':', min(temp[j]), max(temp[j]), len(temp[j]))\n",
    "    print('\\n')\n",
    "\n",
    "    #Convert Target array back to correct shape\n",
    "    target_np=np.ravel(target_np_bin)\n",
    "    \n",
    "\n",
    "#############################################################################\n",
    "#\n",
    "# Feature Selection\n",
    "#\n",
    "##########################################\n",
    "\n",
    "#Low Variance Filter\n",
    "if lv_filter==1:\n",
    "    print('--LOW VARIANCE FILTER ON--', '\\n')\n",
    "    \n",
    "    #LV Threshold\n",
    "    sel = VarianceThreshold(threshold=0.5)                                      #Removes any feature with less than 20% variance\n",
    "    fit_mod=sel.fit(data_np)\n",
    "    fitted=sel.transform(data_np)\n",
    "    sel_idx=fit_mod.get_support()\n",
    "\n",
    "    #Get lists of selected and non-selected features (names and indexes)\n",
    "    temp=[]\n",
    "    temp_idx=[]\n",
    "    temp_del=[]\n",
    "    for i in range(len(data_np[0])):\n",
    "        if sel_idx[i]==1:                                                           #Selected Features get added to temp header\n",
    "            temp.append(header[i+feat_start])\n",
    "            temp_idx.append(i)\n",
    "        else:                                                                       #Indexes of non-selected features get added to delete array\n",
    "            temp_del.append(i)\n",
    "\n",
    "    print('Selected', temp)\n",
    "    print('Features (total, selected):', len(data_np[0]), len(temp))\n",
    "    print('\\n')\n",
    "\n",
    "    #Filter selected columns from original dataset\n",
    "    header = header[0:feat_start]\n",
    "    for field in temp:\n",
    "        header.append(field)\n",
    "    data_np = np.delete(data_np, temp_del, axis=1)                                 #Deletes non-selected features by index\n",
    "\n",
    "\n",
    "#Feature Selection\n",
    "if feat_select==1:\n",
    "    '''Three steps:\n",
    "       1) Run Feature Selection\n",
    "       2) Get lists of selected and non-selected features\n",
    "       3) Filter columns from original dataset\n",
    "       '''\n",
    "    \n",
    "    print('--FEATURE SELECTION ON--', '\\n')\n",
    "    \n",
    "    ##1) Run Feature Selection #######\n",
    "    if fs_type==1:\n",
    "        #Stepwise Recursive Backwards Feature removal\n",
    "        if binning==1:\n",
    "            clf = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split=3, criterion='entropy', random_state=rand_st)\n",
    "            sel = RFE(clf, n_features_to_select=k_cnt, step=.1)\n",
    "            print('Stepwise Recursive Backwards - Random Forest: ')\n",
    "        if binning==0:\n",
    "            rgr = RandomForestRegressor(n_estimators=500, max_depth=None, min_samples_split=3, criterion='mse', random_state=rand_st)\n",
    "            sel = RFE(rgr, n_features_to_select=k_cnt, step=.1)\n",
    "            print('Stepwise Recursive Backwards - Random Forest: ')\n",
    "            \n",
    "        fit_mod=sel.fit(data_np, target_np)\n",
    "        print(sel.ranking_)\n",
    "        sel_idx=fit_mod.get_support()      \n",
    "\n",
    "    if fs_type==2:\n",
    "        #Wrapper Select via model\n",
    "        if binning==1:\n",
    "            clf = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, max_depth=3, min_samples_split=3, random_state=rand_st)\n",
    "            sel = SelectFromModel(clf, prefit=False, threshold='mean', max_features=None)                                                           #to select only based on max_features, set to integer value and set threshold=-np.inf\n",
    "            print ('Wrapper Select: ')\n",
    "        if binning==0:\n",
    "            rgr = GradientBoostingRegressor(loss='ls', learning_rate=0.1, n_estimators=100, min_samples_split=3, max_depth=3, random_state=rand_st)\n",
    "            sel = SelectFromModel(rgr, prefit=False, threshold='mean', max_features=None)\n",
    "            print ('Wrapper Select: ')\n",
    "            \n",
    "        fit_mod=sel.fit(data_np, target_np)    \n",
    "        sel_idx=fit_mod.get_support()\n",
    "\n",
    "    if fs_type==3:\n",
    "        if binning==1:                                                              ######Only work if the Target is binned###########\n",
    "            #Univariate Feature Selection - Chi-squared\n",
    "            sel=SelectKBest(chi2, k=k_cnt)\n",
    "            fit_mod=sel.fit(data_np, target_np)                                         #will throw error if any negative values in features, so turn off feature normalization, or switch to mutual_info_classif\n",
    "            print ('Univariate Feature Selection - Chi2: ')\n",
    "            sel_idx=fit_mod.get_support()\n",
    "\n",
    "        if binning==0:                                                              ######Only work if the Target is continuous###########\n",
    "            #Univariate Feature Selection - Mutual Info Regression\n",
    "            sel=SelectKBest(mutual_info_regression, k=k_cnt)\n",
    "            fit_mod=sel.fit(data_np, target_np)\n",
    "            print ('Univariate Feature Selection - Mutual Info: ')\n",
    "            sel_idx=fit_mod.get_support()\n",
    "\n",
    "        #Print ranked variables out sorted\n",
    "        temp=[]\n",
    "        scores=fit_mod.scores_\n",
    "        for i in range(feat_start, len(header)):            \n",
    "            temp.append([header[i], float(scores[i-feat_start])])\n",
    "\n",
    "        print('Ranked Features')\n",
    "        temp_sort=sorted(temp, key=itemgetter(1), reverse=True)\n",
    "        for i in range(len(temp_sort)):\n",
    "            print(i, temp_sort[i][0], ':', temp_sort[i][1])\n",
    "        print('\\n')\n",
    "\n",
    "    ##2) Get lists of selected and non-selected features (names and indexes) #######\n",
    "    temp=[]\n",
    "    temp_idx=[]\n",
    "    temp_del=[]\n",
    "    for i in range(len(data_np[0])):\n",
    "        if sel_idx[i]==1:                                                           #Selected Features get added to temp header\n",
    "            temp.append(header[i+feat_start])\n",
    "            temp_idx.append(i)\n",
    "        else:                                                                       #Indexes of non-selected features get added to delete array\n",
    "            temp_del.append(i)\n",
    "    print('Selected', temp)\n",
    "    print('Features (total/selected):', len(data_np[0]), len(temp))\n",
    "    print('\\n')\n",
    "            \n",
    "                \n",
    "    ##3) Filter selected columns from original dataset #########\n",
    "    header = header[0:feat_start]\n",
    "    for field in temp:\n",
    "        header.append(field)\n",
    "    data_np = np.delete(data_np, temp_del, axis=1)                                 #Deletes non-selected features by index)\n",
    "    \n",
    "    \n",
    "\n",
    "#############################################################################\n",
    "#\n",
    "# Train SciKit Models\n",
    "#\n",
    "##########################################\n",
    "\n",
    "print('--ML Model Output--', '\\n')\n",
    "\n",
    "#Test/Train split\n",
    "data_train, data_test, target_train, target_test = train_test_split(data_np, target_np, test_size=0.35)\n",
    "\n",
    "####Regressors####\n",
    "if binning==0 and cross_val==0:\n",
    "    #SciKit\n",
    "    '''Test/Train split unused in this homework, skip down to CV section'''\n",
    " \n",
    "\n",
    "                                                                                                                          \n",
    "\n",
    "####Cross-Val Regressors####\n",
    "if binning==0 and cross_val==1:\n",
    "    #Setup Crossval regression scorers\n",
    "    scorers = {'Neg_MSE': 'neg_mean_squared_error', 'expl_var': 'explained_variance'} \n",
    "    \n",
    "    #SciKit Gradient Boosting - Cross Val\n",
    "    start_ts=time.time()\n",
    "    rgr= GradientBoostingRegressor(loss='ls', learning_rate=0.1, n_estimators=100, min_samples_split=3, max_depth=3, random_state=rand_st)\n",
    "    scores= cross_validate(rgr, data_np, target_np, scoring=scorers, cv=5)                                                                                             \n",
    "\n",
    "    scores_RMSE = np.asarray([math.sqrt(-x) for x in scores['test_Neg_MSE']])                                       #Turns negative MSE scores into RMSE\n",
    "    scores_Expl_Var = scores['test_expl_var']\n",
    "    print(\"Gradient Boosting RMSE:: %0.2f (+/- %0.2f)\" % ((scores_RMSE.mean()), (scores_RMSE.std() * 2)))\n",
    "    print(\"Gradient Boosting Expl Var: %0.2f (+/- %0.2f)\" % ((scores_Expl_Var.mean()), (scores_Expl_Var.std() * 2)))\n",
    "    print(\"Gradient Boosting CV Runtime:\", time.time()-start_ts)\n",
    "\n",
    "\n",
    "    #SciKit Ada Boosting - Cross Val\n",
    "    start_ts=time.time()\n",
    "    rgr= AdaBoostRegressor(base_estimator=None, n_estimators=100, learning_rate=0.5, loss='linear', random_state=rand_st)\n",
    "    scores= cross_validate(rgr, data_np, target_np, scoring=scorers, cv=5)                                                                                             \n",
    "\n",
    "    scores_RMSE = np.asarray([math.sqrt(-x) for x in scores['test_Neg_MSE']])                                       #Turns negative MSE scores into RMSE\n",
    "    scores_Expl_Var = scores['test_expl_var']\n",
    "    print(\"AdaBoost RMSE:: %0.2f (+/- %0.2f)\" % ((scores_RMSE.mean()), (scores_RMSE.std() * 2)))\n",
    "    print(\"AdaBoost Expl Var: %0.2f (+/- %0.2f)\" % ((scores_Expl_Var.mean()), (scores_Expl_Var.std() * 2)))\n",
    "    print(\"AdaBoost CV Runtime:\", time.time()-start_ts)\n",
    "\n",
    "\n",
    "    #SciKit Neural Network - Cross Val\n",
    "    start_ts=time.time()\n",
    "    rgr= MLPRegressor(hidden_layer_sizes=(10,), activation='logistic', solver='lbfgs', alpha=0.0001, max_iter=1000, random_state=rand_st)\n",
    "    scores= cross_validate(rgr, data_np, target_np, scoring=scorers, cv=5)                                                                                             \n",
    "\n",
    "    scores_RMSE = np.asarray([math.sqrt(-x) for x in scores['test_Neg_MSE']])                                       #Turns negative MSE scores into RMSE\n",
    "    scores_Expl_Var = scores['test_expl_var']\n",
    "    print(\"Neural Network RMSE:: %0.2f (+/- %0.2f)\" % ((scores_RMSE.mean()), (scores_RMSE.std() * 2)))\n",
    "    print(\"Neural Network Expl Var: %0.2f (+/- %0.2f)\" % ((scores_Expl_Var.mean()), (scores_Expl_Var.std() * 2)))\n",
    "    print(\"Neural Network CV Runtime:\", time.time()-start_ts)\n",
    "\n",
    "\n",
    "\n",
    "####Cross-Val Classifiers####\n",
    "if binning==1 and cross_val==1:\n",
    "    scorers = {'Accuracy': 'accuracy', 'roc_auc': 'roc_auc'}\n",
    "    \n",
    "    start_ts=time.time()\n",
    "    clf= GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, max_depth=3, min_samples_split=3, random_state=rand_st)\n",
    "    scores= cross_validate(clf, data_np, y=target_np, scoring=scorers, cv=5)\n",
    "\n",
    "    scores_Acc = scores['test_Accuracy']                                                                                                                                    \n",
    "    print(\"Gradient Boosting Acc: %0.2f (+/- %0.2f)\" % (scores_Acc.mean(), scores_Acc.std() * 2))                                                                                                    \n",
    "    scores_AUC= scores['test_roc_auc']                                                                     #Only works with binary classes, not multiclass                  \n",
    "    print(\"Gradient Boosting AUC: %0.2f (+/- %0.2f)\" % (scores_AUC.mean(), scores_AUC.std() * 2))                           \n",
    "    print(\"Gradient Boosting CV Runtime:\", time.time()-start_ts)\n",
    "\n",
    "\n",
    "    #SciKit Ada Boosting - Cross Val\n",
    "    start_ts=time.time()\n",
    "    clf= AdaBoostClassifier(base_estimator=None, n_estimators=100, learning_rate=0.1, random_state=rand_st)\n",
    "    scores=  cross_validate(clf, data_np, y=target_np, scoring=scorers, cv=5)\n",
    "\n",
    "    scores_Acc = scores['test_Accuracy']                                                                                                                                    \n",
    "    print(\"AdaBoost Acc: %0.2f (+/- %0.2f)\" % (scores_Acc.mean(), scores_Acc.std() * 2))                                                                                                    \n",
    "    scores_AUC= scores['test_roc_auc']                                                                     #Only works with binary classes, not multiclass                  \n",
    "    print(\"AdaBoost AUC: %0.2f (+/- %0.2f)\" % (scores_AUC.mean(), scores_AUC.std() * 2))                           \n",
    "    print(\"AdaBoost CV Runtime:\", time.time()-start_ts)\n",
    "\n",
    "    #SciKit Neural Network - Cross Val\n",
    "    start_ts=time.time()\n",
    "    clf= MLPClassifier(hidden_layer_sizes= (10,), activation='logistic', solver='adam', alpha=0.0001, max_iter=1000, random_state=rand_st)\n",
    "    scores=  cross_validate(clf, data_np, y=target_np, scoring=scorers, cv=5)\n",
    "\n",
    "    scores_Acc = scores['test_Accuracy']                                                                                                                                    \n",
    "    print(\"Neural Network Acc: %0.2f (+/- %0.2f)\" % (scores_Acc.mean(), scores_Acc.std() * 2))                                                                                                    \n",
    "    scores_AUC= scores['test_roc_auc']                                                                     #Only works with binary classes, not multiclass                  \n",
    "    print(\"Neural Network AUC: %0.2f (+/- %0.2f)\" % (scores_AUC.mean(), scores_AUC.std() * 2))                           \n",
    "    print(\"Neural Network CV Runtime:\", time.time()-start_ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
